{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0q8VPZSioZpo",
    "outputId": "4895409a-95f2-47c1-f439-edc81d2aee45"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import gc\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Seeding for consistency in reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "4GymUcejCzK7"
   },
   "outputs": [],
   "source": [
    "# Courtesy & more details about dataset https://www.kaggle.com/basilb2s/language-detection\n",
    "!wget -q https://raw.githubusercontent.com/maqboolkhan/Project-NLP/master/Classification/Language%20Detection.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5LG-mTFw3uMz",
    "outputId": "6c066ca6-2dfb-4824-85f2-ffe22989a64b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English',\n",
       " 'Malayalam',\n",
       " 'Hindi',\n",
       " 'Tamil',\n",
       " 'Portugeese',\n",
       " 'French',\n",
       " 'Dutch',\n",
       " 'Spanish',\n",
       " 'Greek',\n",
       " 'Russian',\n",
       " 'Danish',\n",
       " 'Italian',\n",
       " 'Turkish',\n",
       " 'Sweedish',\n",
       " 'Arabic',\n",
       " 'German',\n",
       " 'Kannada']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pd.read_csv('Language Detection.csv')\n",
    "\n",
    "# Printing all available languages in the dataset\n",
    "ds.Language.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "hpbApayipKwe"
   },
   "outputs": [],
   "source": [
    "ds = ds.loc[  (ds.Language == 'German') | (ds.Language == 'English') | (ds.Language == 'Arabic')]\n",
    "train_set, test_set = train_test_split(ds, test_size=0.3, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8IZuiQBpK2i",
    "outputId": "6e628e6b-cd9a-4f04-a4af-b8ad21b22099"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English', 'Arabic', 'German']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_langs = ds.Language.unique().tolist()\n",
    "trg_langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "IU5gNHKIpK5J"
   },
   "outputs": [],
   "source": [
    "class LangDataset(Dataset):\n",
    "    def __init__(self, ds, trg_langs, train_vocab=None):\n",
    "        self.corpus = ds\n",
    "\n",
    "        if not train_vocab:\n",
    "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
    "        else:\n",
    "            self.src_vocab, self.trg_vocab = train_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.corpus.iloc[item].Text\n",
    "        lang = self.corpus.iloc[item].Language\n",
    "\n",
    "        return {\n",
    "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
    "            'trg': self.trg_vocab.lookup_indices([lang])\n",
    "        }\n",
    "\n",
    "    def _build_vocab(self):\n",
    "        # Here one could remove stopwords and use word lemmatisation.\n",
    "        # Both techniques will reduce the vocab size and hence model size\n",
    "        # and could also enhance the model's performance \n",
    "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
    "\n",
    "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\", \"<pad>\"])\n",
    "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
    "\n",
    "        trg_vocab = build_vocab_from_iterator([trg_langs])\n",
    "\n",
    "        return src_vocab, trg_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "26Mbt2jzpK7y"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, maxlen = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # A tensor consists of all the possible positions (index) e.g 0, 1, 2, ... max length of input\n",
    "        # Shape (pos) --> [max len, 1]\n",
    "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
    "        pos_encoding = torch.zeros((maxlen, d_model))\n",
    "\n",
    "        sin_den = 10000 ** (torch.arange(0, d_model, 2)/d_model) # sin for even item of position's dimension\n",
    "        cos_den = 10000 ** (torch.arange(1, d_model, 2)/d_model) # cos for odd \n",
    "\n",
    "        pos_encoding[:, 0::2] = torch.sin(pos / sin_den) \n",
    "        pos_encoding[:, 1::2] = torch.cos(pos / cos_den)\n",
    "\n",
    "        # Shape (pos_embedding) --> [max len, d_model]\n",
    "        # Adding one more dimension in-between\n",
    "        pos_encoding = pos_encoding.unsqueeze(-2)\n",
    "        # Shape (pos_embedding) --> [max len, 1, d_model]\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # We want pos_encoding be saved and restored in the `state_dict`, but not trained by the optimizer\n",
    "        # hence registering it!\n",
    "        # Source & credits: https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/2\n",
    "        self.register_buffer('pos_encoding', pos_encoding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        # shape (token_embedding) --> [sentence len, batch size, d_model]\n",
    "\n",
    "        # Concatenating embeddings with positional encodings\n",
    "        # Note: As we made positional encoding with the size max length of sentence in our dataset \n",
    "        #       hence here we are picking till the sentence length in a batch\n",
    "        #       Another thing to notice is in the Transformer's paper they used FIXED positional encoding, \n",
    "        #       there are methods where we can also learn them\n",
    "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
    "\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # shape (tokens) --> [sentence len, batch size]\n",
    "        # shape (inp_emb) --> [sentence len, batch size, d_model]\n",
    "        # Multiplying with square root of d_model as they mentioned in the Transformer's paper\n",
    "        inp_emb = self.embedding(tokens.long()) * math.sqrt(self.d_model)\n",
    "        return inp_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "P9XcPf5opK-d"
   },
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size,\n",
    "                 trg_vocab_size,\n",
    "                 d_model,\n",
    "                 dropout,\n",
    "                 n_head,\n",
    "                 dim_feedforward,\n",
    "                 n_layers\n",
    "                ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_inp_emb = InputEmbedding(src_vocab_size, d_model)\n",
    "        self.trg_inp_emb = InputEmbedding(trg_vocab_size, d_model)\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        # Only using Encoder of Transformer model\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.decoder = nn.Linear(d_model, trg_vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_emb = self.positional_encoding(self.src_inp_emb(x))\n",
    "        # Shape (output) -> (Sequence length, batch size, d_model)\n",
    "        output = self.transformer_encoder(x_emb)\n",
    "        # We want our output to be in the shape of (batch size, d_model) so that\n",
    "        # we can use it with CrossEntropyLoss hence averaging using first (Sequence length) dimension \n",
    "        # Shape (mean) -> (batch size, d_model)\n",
    "        # Shape (decoder) -> (batch size, d_model)\n",
    "        return self.decoder(output.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "JO1HCcF0pLBH"
   },
   "outputs": [],
   "source": [
    "hyp_params = {\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.0005,\n",
    "    \"num_epochs\": 10,\n",
    "    \"d_model\": 512, # Input embedding dimension\n",
    "    \"n_head\": 8, # No. of multi-head attention block (aka paralle self-attention layers)\n",
    "    \"n_layers\": 3,\n",
    "    \"feedforward_dim\": 128,\n",
    "    \"dropout\": 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "a-HYIFzApLD2"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch, pad_value, device):\n",
    "    trgs = []\n",
    "    srcs = []\n",
    "    for row in batch:\n",
    "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
    "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
    "\n",
    "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
    "    return {\"src\": padded_srcs, \"trg\": torch.tensor([trgs]).to(device)}\n",
    "\n",
    "train_langds = LangDataset(train_set, trg_langs)\n",
    "test_langds = LangDataset(test_set, trg_langs, (train_langds.src_vocab, train_langds.trg_vocab))\n",
    "\n",
    "SRC_PAD_IDX = train_langds.src_vocab[\"<pad>\"]\n",
    "\n",
    "train_dt = DataLoader(train_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "test_dt = DataLoader(test_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
    "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, SRC_PAD_IDX, device))\n",
    "\n",
    "hyp_params[\"src_vocab_size\"] = len(train_langds.src_vocab)\n",
    "hyp_params[\"trg_vocab_size\"] = len(trg_langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "yO-YAr58pLGq"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "        # Clear the accumulating gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        src = batch[\"src\"]  # shape --> [seq len, batch size]\n",
    "        trg = batch[\"trg\"]  # shape --> [1, batch size]\n",
    "\n",
    "        # shape (out) --> [batch size, trg size]\n",
    "        out = model(src)\n",
    "        loss = criterion(out, trg.squeeze(0))\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().cpu()\n",
    "\n",
    "    return epoch_loss/len(train_dataloader)\n",
    "\n",
    "\n",
    "def evaluate_model(model, valid_dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(valid_dataloader):\n",
    "            src = batch[\"src\"]  # shape --> [seq len, batch size]\n",
    "            trg = batch[\"trg\"]  # shape --> [1, batch size]\n",
    "\n",
    "            # shape (out) --> [batch size, trg size]\n",
    "            out = model(src)\n",
    "            loss = criterion(out, trg.squeeze(0))\n",
    "\n",
    "            epoch_loss += loss.detach().cpu()\n",
    "\n",
    "    return epoch_loss/len(valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "i8xBMgdipLJi"
   },
   "outputs": [],
   "source": [
    "model = TransformerClassifier(hyp_params[\"src_vocab_size\"],\n",
    "                                hyp_params[\"trg_vocab_size\"],\n",
    "                                hyp_params[\"d_model\"],\n",
    "                                hyp_params[\"dropout\"],\n",
    "                                hyp_params[\"n_head\"],\n",
    "                                hyp_params[\"feedforward_dim\"],\n",
    "                                hyp_params[\"n_layers\"]\n",
    "                                ).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=hyp_params[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qqsaS8XvpLNR",
    "outputId": "6d66defa-ce9e-49a0-d67c-1958210f250e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.52770, Eval loss: 0.12146. Time 4.08 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.08631, Eval loss: 0.10422. Time 3.85 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 0.03560, Eval loss: 0.10591. Time 3.86 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 0.02582, Eval loss: 0.09553. Time 4.01 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.03062, Eval loss: 0.13160. Time 4.04 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 0.02317, Eval loss: 0.13430. Time 3.90 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 0.02874, Eval loss: 0.10321. Time 3.89 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 0.02292, Eval loss: 0.13102. Time 4.87 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 0.02401, Eval loss: 0.13358. Time 4.22 secs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27/27 [00:03<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.03079, Eval loss: 0.16248. Time 3.93 secs\n"
     ]
    }
   ],
   "source": [
    "min_el = math.inf\n",
    "patience = 1\n",
    "best_model = {}\n",
    "best_epoch = 0\n",
    "\n",
    "epoch_loss = 0\n",
    "for epoch in range(hyp_params[\"num_epochs\"]):\n",
    "  start = time.time()\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  epoch_loss = train_model(model, train_dt, criterion, optimizer)\n",
    "  eval_loss = evaluate_model(model, test_dt, criterion)\n",
    "  \n",
    "  \n",
    "  print(f\"Epoch: {epoch+1}, Train loss: {epoch_loss:.5f}, Eval loss: {eval_loss:.5f}. Time {time.time() - start:.2f} secs\")\n",
    "\n",
    "  if eval_loss < min_el:\n",
    "      best_epoch = epoch+1\n",
    "      min_el = eval_loss\n",
    "      best_model = copy.deepcopy(model)\n",
    "      # torch.save({\n",
    "      #     'model_state_dict': model.state_dict(),\n",
    "      #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "      #     'eval_loss': min_el\n",
    "      # }, 'model-transformer.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "1xVA9aIxuZjD",
    "outputId": "205e1470-d346-4417-a4e1-6329e186aa59"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Best epoch was 4 with 0.09552959352731705 eval loss'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Best epoch was {best_epoch} with {min_el} eval loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "9a3WD73OuS7C"
   },
   "outputs": [],
   "source": [
    "true_labels =[]\n",
    "pred_labels =[]\n",
    "\n",
    "for i in test_langds:\n",
    "    inp = torch.tensor(i['src']).unsqueeze(1).to(device)\n",
    "    trg = i['trg'][0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = best_model(inp).view(-1).argmax().item()\n",
    "\n",
    "    true_labels.append(trg)\n",
    "    pred_labels.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVotnS1tB-9N",
    "outputId": "c081fadf-4331-4d4a-c97c-40c8730e42fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.89      0.92       167\n",
      "           1       0.99      0.97      0.98       417\n",
      "           2       0.86      0.97      0.91       134\n",
      "\n",
      "    accuracy                           0.95       718\n",
      "   macro avg       0.93      0.94      0.94       718\n",
      "weighted avg       0.95      0.95      0.95       718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPtX9SAFRIXM"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "w6czHogCRKTo",
    "outputId": "71a842aa-a164-4dfc-c61a-a27c7f545d68"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'German'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"hallo, wie gehts ihnen?\"\n",
    "txt_to_ind = train_langds.src_vocab.lookup_indices(text.split())\n",
    "inp_tensor = torch.tensor(txt_to_ind).to(device).unsqueeze(1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = best_model(inp_tensor).view(-1).argmax().item()\n",
    "train_langds.trg_vocab.lookup_token(res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Classification with Transformer Pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
