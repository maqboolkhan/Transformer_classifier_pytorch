{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification with Transformer Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0q8VPZSioZpo",
        "outputId": "8c136a2c-919f-4df3-f471-6dd53abca94e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "import math\n",
        "import gc\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Seeding for consistency in reproducibility\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Courtesy & more details about dataset https://www.kaggle.com/basilb2s/language-detection\n",
        "!wget -q https://raw.githubusercontent.com/maqboolkhan/Project-NLP/master/Classification/Language%20Detection.csv"
      ],
      "metadata": {
        "id": "4GymUcejCzK7"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = pd.read_csv('Language Detection.csv')\n",
        "\n",
        "# Printing all available languages in the dataset\n",
        "ds.Language.unique().tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LG-mTFw3uMz",
        "outputId": "5eb80451-d185-4366-8525-5f2c0319f5c7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['English',\n",
              " 'Malayalam',\n",
              " 'Hindi',\n",
              " 'Tamil',\n",
              " 'Portugeese',\n",
              " 'French',\n",
              " 'Dutch',\n",
              " 'Spanish',\n",
              " 'Greek',\n",
              " 'Russian',\n",
              " 'Danish',\n",
              " 'Italian',\n",
              " 'Turkish',\n",
              " 'Sweedish',\n",
              " 'Arabic',\n",
              " 'German',\n",
              " 'Kannada']"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds = ds.loc[  (ds.Language == 'German') | (ds.Language == 'Dutch')]\n",
        "train_set, test_set = train_test_split(ds, test_size=0.3, random_state=2022)"
      ],
      "metadata": {
        "id": "hpbApayipKwe"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trg_langs = ds.Language.unique().tolist()\n",
        "trg_langs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8IZuiQBpK2i",
        "outputId": "8daff15a-f0cd-45a9-d657-28baa749e619"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dutch', 'German']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LangDataset(Dataset):\n",
        "    def __init__(self, ds, trg_langs, train_vocab=None):\n",
        "        self.corpus = ds\n",
        "\n",
        "        if not train_vocab:\n",
        "            self.src_vocab, self.trg_vocab = self._build_vocab()\n",
        "        else:\n",
        "            self.src_vocab, self.trg_vocab = train_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        text = self.corpus.iloc[item].Text\n",
        "        lang = self.corpus.iloc[item].Language\n",
        "\n",
        "        return {\n",
        "            'src': self.src_vocab.lookup_indices(text.lower().split()),\n",
        "            'trg': self.trg_vocab.lookup_indices([lang])\n",
        "        }\n",
        "\n",
        "    def _build_vocab(self):\n",
        "        src_tokens = self.corpus.Text.str.cat().lower().split()\n",
        "\n",
        "        src_vocab = build_vocab_from_iterator([src_tokens], specials=[\"<unk>\", \"<pad>\"])\n",
        "        src_vocab.set_default_index(src_vocab['<unk>'])\n",
        "\n",
        "        trg_vocab = build_vocab_from_iterator([trg_langs])\n",
        "\n",
        "        return src_vocab, trg_vocab"
      ],
      "metadata": {
        "id": "IU5gNHKIpK5J"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, maxlen = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # A tensor consists of all the possible positions (index) e.g 0, 1, 2, ... max length of input\n",
        "        # Shape (pos) --> [max len, 1]\n",
        "        pos = torch.arange(0, maxlen).unsqueeze(1)\n",
        "        pos_encoding = torch.zeros((maxlen, d_model))\n",
        "\n",
        "        # In the paper, they had 2i in the positional encoding formula\n",
        "        # where i is the dimension \n",
        "        sin_den = 10000 ** (torch.arange(0, d_model, 2)/d_model) # sin for even item of position's dimension\n",
        "        cos_den = 10000 ** (torch.arange(1, d_model, 2)/d_model) # cos for odd \n",
        "\n",
        "        pos_encoding[:, 0::2] = torch.sin(pos / sin_den) \n",
        "        pos_encoding[:, 1::2] = torch.cos(pos / cos_den)\n",
        "\n",
        "        # Shape (pos_embedding) --> [max len, d_model]\n",
        "        pos_encoding = pos_encoding.unsqueeze(-2)\n",
        "        # Shape (pos_embedding) --> [max len, 1, d_model]\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # We want pos_encoding be saved and restored in the `state_dict`, but not trained by the optimizer\n",
        "        # hence registering it!\n",
        "        # Source & credits: https://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723/2\n",
        "        self.register_buffer('pos_encoding', pos_encoding)\n",
        "\n",
        "    def forward(self, token_embedding):\n",
        "        # shape (token_embedding) --> [sentence len, batch size, d_model]\n",
        "\n",
        "        # Concatenating embeddings with positional encodings\n",
        "        # Note: As we made positional encoding with the size max length of sentence in our dataset \n",
        "        #       hence here we are picking till the sentence length in a batch\n",
        "        #       Another thing to notice is in the Transformer's paper they used FIXED positional encoding, \n",
        "        #       there are methods where we can also learn them\n",
        "        return self.dropout(token_embedding + self.pos_encoding[:token_embedding.size(0), :])\n",
        "\n",
        "\n",
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        super(InputEmbedding, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        # shape (tokens) --> [sentence len, batch size]\n",
        "        # shape (inp_emb) --> [sentence len, batch size, d_model]\n",
        "        # Multiplying with square root of d_model as they mentioned in the Transformer's paper\n",
        "        inp_emb = self.embedding(tokens.long()) * math.sqrt(self.d_model)\n",
        "        return inp_emb"
      ],
      "metadata": {
        "id": "26Mbt2jzpK7y"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self,\n",
        "                 src_vocab_size,\n",
        "                 trg_vocab_size,\n",
        "                 d_model,\n",
        "                 dropout,\n",
        "                 n_head,\n",
        "                 dim_feedforward,\n",
        "                 n_layers\n",
        "                ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.src_inp_emb = InputEmbedding(src_vocab_size, d_model)\n",
        "        self.trg_inp_emb = InputEmbedding(trg_vocab_size, d_model)\n",
        "\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout=dropout)\n",
        "\n",
        "        # Only using Encoder of Transformer model\n",
        "        encoder_layers = nn.TransformerEncoderLayer(d_model, n_head, dim_feedforward, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, trg_vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_emb = self.positional_encoding(self.src_inp_emb(x))\n",
        "        # Shape (output) -> (Sequence length, batch size, d_model)\n",
        "        output = self.transformer_encoder(x_emb)\n",
        "        # We want our output to be in the shape of (batch size, d_model) so that\n",
        "        # we can use it with CrossEntropyLoss hence averaging using first (Sequence length) dimension \n",
        "        # Shape (mean) -> (batch size, d_model)\n",
        "        # Shape (decoder) -> (batch size, d_model)\n",
        "        return self.decoder(output.mean(0))"
      ],
      "metadata": {
        "id": "P9XcPf5opK-d"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyp_params = {\n",
        "    \"batch_size\": 64,\n",
        "    \"lr\": 0.0005,\n",
        "    \"num_epochs\": 10,\n",
        "    \"d_model\": 512, # Input embedding dimension\n",
        "    \"n_head\": 8, # No. of multi-head attention block (aka paralle self-attention layers)\n",
        "    \"n_layers\": 3,\n",
        "    \"feedforward_dim\": 128,\n",
        "    \"dropout\": 0.1\n",
        "}"
      ],
      "metadata": {
        "id": "JO1HCcF0pLBH"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch, pad_value, device):\n",
        "    trgs = []\n",
        "    srcs = []\n",
        "    for row in batch:\n",
        "        srcs.append(torch.tensor(row[\"src\"], dtype=torch.long).to(device))\n",
        "        trgs.append(torch.tensor(row[\"trg\"]).to(device))\n",
        "\n",
        "    padded_srcs = pad_sequence(srcs, padding_value=pad_value)\n",
        "    return {\"src\": padded_srcs, \"trg\": torch.tensor([trgs]).to(device)}\n",
        "\n",
        "train_langds = LangDataset(train_set, trg_langs)\n",
        "test_langds = LangDataset(test_set, trg_langs, (train_langds.src_vocab, train_langds.trg_vocab))\n",
        "\n",
        "SRC_PAD_IDX = train_langds.src_vocab[\"<pad>\"]\n",
        "\n",
        "train_dt = DataLoader(train_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
        "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, SRC_PAD_IDX, device))\n",
        "\n",
        "test_dt = DataLoader(test_langds, batch_size=hyp_params[\"batch_size\"], shuffle=\n",
        "                   True, collate_fn=lambda batch_size: collate_fn(batch_size, SRC_PAD_IDX, device))\n",
        "\n",
        "hyp_params[\"src_vocab_size\"] = len(train_langds.src_vocab)\n",
        "hyp_params[\"trg_vocab_size\"] = len(trg_langs)"
      ],
      "metadata": {
        "id": "a-HYIFzApLD2"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_idx, batch in enumerate(tqdm(train_dataloader)):\n",
        "        # Clear the accumulating gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src = batch[\"src\"]  # shape --> [seq len, batch size]\n",
        "        trg = batch[\"trg\"]  # shape --> [1, batch size]\n",
        "\n",
        "        # shape (out) --> [batch size, trg size]\n",
        "        out = model(src)\n",
        "        loss = criterion(out, trg.squeeze(0))\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate_model(model, valid_dataloader, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(valid_dataloader):\n",
        "            src = batch[\"src\"]  # shape --> [seq len, batch size]\n",
        "            trg = batch[\"trg\"]  # shape --> [1, batch size]\n",
        "\n",
        "            # shape (out) --> [batch size, trg size]\n",
        "            out = model(src)\n",
        "            loss = criterion(out, trg.squeeze(0))\n",
        "\n",
        "            epoch_loss += loss.detach().cpu()\n",
        "\n",
        "    return epoch_loss/len(valid_dataloader)"
      ],
      "metadata": {
        "id": "yO-YAr58pLGq"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TransformerClassifier(hyp_params[\"src_vocab_size\"],\n",
        "                                hyp_params[\"trg_vocab_size\"],\n",
        "                                hyp_params[\"d_model\"],\n",
        "                                hyp_params[\"dropout\"],\n",
        "                                hyp_params[\"n_head\"],\n",
        "                                hyp_params[\"feedforward_dim\"],\n",
        "                                hyp_params[\"n_layers\"]\n",
        "                                ).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyp_params[\"lr\"])"
      ],
      "metadata": {
        "id": "i8xBMgdipLJi"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_el = math.inf\n",
        "patience = 1\n",
        "best_model = {}\n",
        "best_epoch = 0\n",
        "\n",
        "epoch_loss = 0\n",
        "for epoch in range(hyp_params[\"num_epochs\"]):\n",
        "  start = time.time()\n",
        "  gc.collect()\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  epoch_loss = train_model(model, train_dt, criterion, optimizer)\n",
        "  eval_loss = evaluate_model(model, test_dt, criterion)\n",
        "  \n",
        "  \n",
        "  print(f\"Epoch: {epoch+1}, Train loss: {epoch_loss:.5f}, Eval loss: {eval_loss:.5f}. Time {time.time() - start:.2f} secs\")\n",
        "\n",
        "  if eval_loss < min_el:\n",
        "      best_epoch = epoch+1\n",
        "      min_el = eval_loss\n",
        "      best_model = copy.deepcopy(model)\n",
        "      # torch.save({\n",
        "      #     'model_state_dict': model.state_dict(),\n",
        "      #     'optimizer_state_dict': optimizer.state_dict(),\n",
        "      #     'eval_loss': min_el\n",
        "      # }, 'model-transformer.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqsaS8XvpLNR",
        "outputId": "c7d729fd-c612-4afd-8d6c-c8ed4e3fba86"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  7.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 0.84613, Eval loss: 0.23620. Time 2.43 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 0.13109, Eval loss: 0.08092. Time 1.69 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 0.06120, Eval loss: 0.04549. Time 1.72 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train loss: 0.03374, Eval loss: 0.10667. Time 1.71 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Train loss: 0.04173, Eval loss: 0.05071. Time 1.72 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Train loss: 0.03065, Eval loss: 0.06671. Time 1.69 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Train loss: 0.02679, Eval loss: 0.06978. Time 1.67 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Train loss: 0.02446, Eval loss: 0.05691. Time 1.70 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00, 10.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Train loss: 0.02317, Eval loss: 0.05605. Time 1.59 secs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [00:01<00:00,  9.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Train loss: 0.03389, Eval loss: 0.06959. Time 1.85 secs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f\"Best epoch was {best_epoch} with {min_el} eval loss\""
      ],
      "metadata": {
        "id": "1xVA9aIxuZjD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "1e5d2613-02df-46a4-ca0b-9bb49afa96e8"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Best epoch was 3 with 0.04549018293619156 eval loss'"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels =[]\n",
        "pred_labels =[]\n",
        "\n",
        "for i in test_langds:\n",
        "    inp = torch.tensor(i['src']).unsqueeze(1).to(device)\n",
        "    trg = i['trg'][0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred = best_model(inp).view(-1).argmax().item()\n",
        "\n",
        "    true_labels.append(trg)\n",
        "    pred_labels.append(pred)"
      ],
      "metadata": {
        "id": "9a3WD73OuS7C"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(true_labels, pred_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVotnS1tB-9N",
        "outputId": "4e01419e-e38f-474b-e7a2-9d3943dcf714"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.96      0.96       162\n",
            "           1       0.95      0.97      0.96       143\n",
            "\n",
            "    accuracy                           0.96       305\n",
            "   macro avg       0.96      0.96      0.96       305\n",
            "weighted avg       0.96      0.96      0.96       305\n",
            "\n"
          ]
        }
      ]
    }
  ]
}